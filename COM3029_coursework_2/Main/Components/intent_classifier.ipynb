{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"intent_classifier.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"NJPc-6SLAUpb"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3sURMQJ806H","executionInfo":{"status":"ok","timestamp":1653207699120,"user_tz":-60,"elapsed":18334,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}},"outputId":"754c7e3c-77fd-42f3-85c4-5f291d4aba83"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DgZfk0st1Bb","outputId":"11bf6d76-4083-4739-fae9-a25a8d6d8842","executionInfo":{"status":"ok","timestamp":1653207712388,"user_tz":-60,"elapsed":8074,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 57.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]}]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import re\n","import pandas as pd\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertModel\n","from torch import nn\n","from torch.optim import Adam"],"metadata":{"id":"l69B3Bq9tcfV","executionInfo":{"status":"ok","timestamp":1653207721735,"user_tz":-60,"elapsed":9352,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# to change the dataset for training, change this.\n","# works best with uni_intents_balanced.csv\n","\n","#DATASET = \"dialogue flow management.csv\"\n","DATASET = \"uni_intents_balanced.csv\""],"metadata":{"id":"TSjKGP2CHI6d","executionInfo":{"status":"ok","timestamp":1653207803071,"user_tz":-60,"elapsed":329,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"cb5pATTrAQDc"}},{"cell_type":"code","source":["#helper code\n","\n","code_names = {\n","    -1 : \"Unknown\",\n","    0 : \"Greeting\",\n","    1 : \"Name\",\n","    2 : \"Address\",\n","    3 : \"DeliveryTime\",\n","    4 : \"NumberOfPizzas\",\n","    5 : \"AddPizza\",\n","    6 : \"RemovePizza\",\n","    7 : \"EditPizza\",\n","    8 : \"AddTopping\",\n","    9 : \"RemoveTopping\",\n","    10 : \"AddSide\",\n","    11 : \"RemoveSide\",\n","    12 : \"AddDrink\",\n","    13 : \"RemoveDrink\",\n","    14 : \"EndConv\",\n","    15 : \"Unknown\" # for BERT bug that doesn't allow -1\n","}\n","\n","gpu = 0\n","device = torch.device(gpu if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    torch.cuda.set_device(gpu)\n","\n","\n","def change_negative(x):\n","  if x < 0:\n","    return int(14 + abs(x))\n","  else:\n","    return x\n"],"metadata":{"id":"XpnWfMpNcLlf","executionInfo":{"status":"ok","timestamp":1653207803311,"user_tz":-60,"elapsed":7,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Data Loading"],"metadata":{"id":"4XoX4oHLADOY"}},{"cell_type":"code","source":["# read dataset\n","data = pd.read_csv(DATASET)\n","data[\"category\"] = data[\"intent_code\"].apply(lambda x: code_names[x])\n","\n","# change unknown -1 to 15\n","\n","data[\"category\"] = data[\"intent_code\"].apply(lambda x: code_names[x])\n","\n","\n","data[\"intent_code\"] = data[\"intent_code\"].apply(lambda x: change_negative(x))\n","data = data.sort_values(by=\"intent_code\")\n","\n","def alphanumericize(x): # and space\n","    text = re.sub(r'[^A-Za-z0-9 ]+', '', x)\n","    return text\n","\n","# lower the text\n","data[\"input\"] = data[\"input\"].apply(lambda x: x.lower())\n","# Clean the text\n","data[\"input\"] = data[\"input\"].apply(lambda x: alphanumericize(x))\n","\n","PREPROCESSING = f\"{str.lower}, {alphanumericize} \""],"metadata":{"id":"xWfch8MXas9D","executionInfo":{"status":"ok","timestamp":1653208459906,"user_tz":-60,"elapsed":245,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{"id":"E9Nk2DggUWNq","executionInfo":{"status":"ok","timestamp":1653208462321,"user_tz":-60,"elapsed":236,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"outputs":[],"source":["# hyperparameters\n","\n","TEST_SPLIT = 0.1\n","VAL_SPLIT = 0.1\n","\n","\n","embed_size = 300 # how big is each word vector\n","max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n","maxlen = 750 # max number of words in a question to use\n","LEARN_RATE = 0.001\n","BATCH_SIZE = 2 # how many samples to process at once\n","N_EPOCHS = 45 # how many times to iterate over all samples\n","n_splits = 5 # Number of K-fold Splits\n","SEED = 48\n","debug = 0"]},{"cell_type":"markdown","source":["\n","----\n","**CREDIT TO:**\n","*    **Author**: Ruben Winastwan\n","*    **Title**: Text Classification with BERT in PyTorch\n","*    **Date**: 30/04/2022\n","*    **Availability**: https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n","*    **LICENSE**: Apache 2.0 open source\n","----"],"metadata":{"id":"PNiISwx-yfWw"}},{"cell_type":"code","execution_count":64,"metadata":{"id":"Fc-Mh27jUWNp","executionInfo":{"status":"ok","timestamp":1653208465072,"user_tz":-60,"elapsed":268,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"outputs":[],"source":["\n","\n","class BertClassifier(nn.Module):\n","      def __init__(self, dropout=0.5):\n","\n","        super(BertClassifier, self).__init__()\n","        n_classes = len(labels)\n","\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, n_classes)\n","        self.relu = nn.ReLU()\n","    \n","\n","      def forward(self, input_id, mask):\n","\n","        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        final_layer = self.relu(linear_output)\n","\n","        return final_layer\n","\n"]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","labels =  {v: k for k, v in code_names.items()}\n","\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","\n","        self.labels = [labels[label] for label in df['category']]\n","        self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for text in df['input']]\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return np.array(self.labels[idx])\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of inputs\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        return batch_texts, batch_y"],"metadata":{"id":"ua_0PsiaYLA2","executionInfo":{"status":"ok","timestamp":1653208466794,"user_tz":-60,"elapsed":1457,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["##\n","from sklearn.model_selection import train_test_split\n","#data = data.sample(frac=1).reset_index(drop=True)  # probably not needed due to stratified sampling\n","\n","# function used twice to make second split\n","train_X, tv_X, train_y, tv_y = train_test_split(data['input'], data['intent_code'],\n","                                                    stratify=data['intent_code'], \n","                                                    test_size=TEST_SPLIT + VAL_SPLIT, random_state=SEED) # e.g. together 0.4\n","new_test_split = TEST_SPLIT/(TEST_SPLIT + VAL_SPLIT) \n","test_X, valid_X, test_y, valid_y = train_test_split(tv_X, tv_y,\n","                                                    stratify=tv_y, \n","                                                    test_size=new_test_split, random_state=SEED)\n","\n","# this is for fixing a weird bug with the splits\n","if test_X.shape[0] == valid_X.shape[0] - 1:\n","    test_X = test_X.append(pd.Series(\"hey you all\"))\n","    test_y = test_y.append(pd.Series(0))\n","\n","# this is to improve intent classifier performance when used with NER\n","train_X = train_X.append(pd.Series(\"name\")).append(pd.Series(\"postcode\")).append(pd.Series(\"time\"))\n","train_y = train_y.append(pd.Series(1)).append(pd.Series(2)).append(pd.Series(3))\n","train_X = train_X.append(pd.Series(\"add topping\")).append(pd.Series(\"remove topping\"))\n","train_y = train_y.append(pd.Series(8)).append(pd.Series(9))\n","train_X = train_X.append(pd.Series(\"add side\")).append(pd.Series(\"remove side\"))\n","train_y = train_y.append(pd.Series(10)).append(pd.Series(11))\n","train_X = train_X.append(pd.Series(\"add drink\")).append(pd.Series(\"remove drink\"))\n","train_y = train_y.append(pd.Series(12)).append(pd.Series(13))\n","###\n","\n","train_data = pd.DataFrame(zip(train_X, train_y), columns=['input', 'intent_code'])\n","train_data[\"category\"] = train_data[\"intent_code\"].apply(lambda x: code_names[x])\n","val_data = pd.DataFrame(zip(valid_X, valid_y), columns=['input', 'intent_code'])\n","val_data[\"category\"] = val_data[\"intent_code\"].apply(lambda x: code_names[x])\n","test_data = pd.DataFrame(zip(test_X, test_y), columns=['input', 'intent_code'])\n","test_data[\"category\"] = test_data[\"intent_code\"].apply(lambda x: code_names[x])\n","\n","train, val, test = Dataset(train_data), Dataset(val_data), Dataset(test_data)\n","\n","# Create Data Loaders\n","train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)"],"metadata":{"id":"XbPPWu2pa87s","executionInfo":{"status":"ok","timestamp":1653208466796,"user_tz":-60,"elapsed":8,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SzfI3SZeUWNr","outputId":"86c4d0f3-88f5-436a-bd79-f6a514e07f1e","executionInfo":{"status":"ok","timestamp":1653208469218,"user_tz":-60,"elapsed":2428,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["#\n","LEARN_RATE =  2e-5\n","\n","model = BertClassifier()\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)"]},{"cell_type":"code","source":["#\n","N_EPOCHS = 8\n","\n","train_loss = []\n","valid_loss = []\n","\n","model = BertClassifier()\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n","\n","\n","for epoch_num in range(N_EPOCHS):\n","\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        for train_input, train_label in (train_loader):\n","\n","            train_label = train_label.to(device)\n","            mask = train_input['attention_mask'].to(device)\n","            input_id = train_input['input_ids'].squeeze(1).to(device)\n","\n","            output = model(input_id, mask)\n","            \n","            batch_loss = criterion(output, train_label)\n","            total_loss_train += batch_loss.item()\n","            \n","            acc = (output.argmax(dim=1) == train_label).sum().item()\n","            total_acc_train += acc\n","\n","            model.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","        \n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        with torch.no_grad():\n","\n","            for val_input, val_label in valid_loader:\n","\n","                val_label = val_label.to(device)\n","                mask = val_input['attention_mask'].to(device)\n","                input_id = val_input['input_ids'].squeeze(1).to(device)\n","\n","                output = model(input_id, mask)\n","\n","                batch_loss = criterion(output, val_label)\n","                total_loss_val += batch_loss.item()\n","                \n","                acc = (output.argmax(dim=1) == val_label).sum().item()\n","                total_acc_val += acc\n","        \n","        train_loss.append(total_loss_train / len(train_data))\n","        valid_loss.append(total_loss_val / len(val_data))\n","        print(\n","            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n","        \n","total_acc_test = 0\n","total_loss_test = 0\n","\n","with torch.no_grad():\n","    for test_input, test_label in test_loader:\n","\n","        test_label = test_label.to(device)\n","        mask = test_input['attention_mask'].to(device)\n","        input_id = test_input['input_ids'].squeeze(1).to(device)\n","\n","        output = model(input_id, mask)\n","\n","        batch_loss = criterion(output, test_label)\n","        total_loss_test += batch_loss.item()\n","        \n","        acc = (output.argmax(dim=1) == test_label).sum().item()\n","        total_acc_test += acc\n","\n","test_loss = total_loss_test / len(test_data)\n","test_accuracy = total_acc_test / len(test_data)\n","\n","print(f\"Test Loss: {test_loss:.2f}, Test Accuracy: {test_accuracy:.2f}\") \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSCUQx7uu_ze","outputId":"fd950f37-b92c-4730-9673-482fb5e291dc","executionInfo":{"status":"ok","timestamp":1653208578369,"user_tz":-60,"elapsed":109158,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 1 | Train Loss:  1.397 | Train Accuracy:  0.093 | Val Loss:  1.384 | Val Accuracy:  0.125\n","Epochs: 2 | Train Loss:  1.315 | Train Accuracy:  0.202 | Val Loss:  1.238 | Val Accuracy:  0.250\n","Epochs: 3 | Train Loss:  1.095 | Train Accuracy:  0.394 | Val Loss:  0.913 | Val Accuracy:  0.583\n","Epochs: 4 | Train Loss:  0.767 | Train Accuracy:  0.668 | Val Loss:  0.579 | Val Accuracy:  0.792\n","Epochs: 5 | Train Loss:  0.416 | Train Accuracy:  0.927 | Val Loss:  0.353 | Val Accuracy:  0.917\n","Epochs: 6 | Train Loss:  0.202 | Train Accuracy:  0.984 | Val Loss:  0.171 | Val Accuracy:  0.958\n","Epochs: 7 | Train Loss:  0.113 | Train Accuracy:  0.990 | Val Loss:  0.114 | Val Accuracy:  0.958\n","Epochs: 8 | Train Loss:  0.070 | Train Accuracy:  1.000 | Val Loss:  0.102 | Val Accuracy:  0.958\n","Test Loss: 0.29, Test Accuracy: 0.79\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"dVh6Ke6Q_ZrQ"}},{"cell_type":"code","source":["import pickle\n","pickle.dump(model, open('intent_classifier.pkl','wb'))"],"metadata":{"id":"gnEjeXlIOfuL","executionInfo":{"status":"ok","timestamp":1653208579547,"user_tz":-60,"elapsed":1196,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["import shutil\n","shutil.copy('intent_classifier.pkl','drive/MyDrive/COM3029 coursework 2/Main/Components/intent_classifier.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MteWWM8hAV9M","executionInfo":{"status":"ok","timestamp":1653208581391,"user_tz":-60,"elapsed":1848,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}},"outputId":"b36a90e5-2739-42aa-c6a2-dd2f1e05a221"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'drive/MyDrive/COM3029 coursework 2/Main/Components/intent_classifier.pkl'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["# Prediction"],"metadata":{"id":"E1BlUaeO_IFb"}},{"cell_type":"code","source":["# this is the old code\n","def predict_intent(input):\n","\n","    test_data = pd.DataFrame(zip([input], [0]), columns=['input', 'intent_code'])\n","    test_data[\"category\"] = test_data[\"intent_code\"].apply(lambda x: code_names[x])\n","\n","    test = Dataset(test_data)\n","\n","    test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    with torch.no_grad():\n","\n","        for test_input, test_label in test_loader:\n","\n","            test_label = test_label.to(device)\n","            mask = test_input['attention_mask'].to(device)\n","            input_id = test_input['input_ids'].squeeze(1).to(device)\n","\n","            output = model(input_id, mask)\n","    \n","    prediction = output.argmax(dim=1).item()\n","\n","    if prediction == 15:\n","      prediction = -1\n","\n","    return prediction\n","\n","predict_intent(\"postcode\")"],"metadata":{"id":"Rc6Q6fEovU9w","executionInfo":{"status":"ok","timestamp":1653208747948,"user_tz":-60,"elapsed":218,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a018addf-cbb2-45c5-a863-855434bc41dc"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["import pickle\n","intent_classifier = pickle.load(open('drive/MyDrive/COM3029 coursework 2/Main/Components/intent_classifier.pkl', 'rb'))"],"metadata":{"id":"qG6NbZfJ7OEt","executionInfo":{"status":"ok","timestamp":1653208837529,"user_tz":-60,"elapsed":1439,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["def get_intent(input):\n","    test_data = pd.DataFrame(zip([input], [0]), columns=['input', 'intent_code'])\n","    test_data[\"category\"] = test_data[\"intent_code\"].apply(lambda x: code_names[x])\n","    test = Dataset(test_data)\n","    test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n","    with torch.no_grad():\n","\n","        for test_input, test_label in test_loader:\n","\n","            test_label = test_label.to(\"cuda:0\")\n","            mask = test_input['attention_mask'].to(\"cuda:0\")\n","            input_id = test_input['input_ids'].squeeze(1).to(\"cuda:0\")\n","            output = intent_classifier(input_id, mask)\n","    \n","    prediction = output.argmax(dim=1).item()\n","\n","    if prediction == 15:\n","      prediction = -1\n","\n","    return prediction\n","print(f\"postcode: {get_intent('postcode')}\")\n","print(f\"time: {get_intent('time')}\")\n","print(f\"add side: {get_intent('add side')}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ImDfE3T7Dhf","executionInfo":{"status":"ok","timestamp":1653209058532,"user_tz":-60,"elapsed":200,"user":{"displayName":"Vida Sharifian","userId":"14228032457063441292"}},"outputId":"a3fa2766-7c3e-4276-d549-8faec3175174"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["postcode: 2\n","time: 3\n","add side: 10\n"]}]}]}